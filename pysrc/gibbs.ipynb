{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import gamma\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm \n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "\n",
    "\n",
    "data_dir = parent_dir+\"/data/calibration/hmc\"\n",
    "site_gamma_df = gpd.read_file(data_dir + \"/gamma_reg_site_1043.geojson\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gibbs Sampling Progress: 100%|██████████| 100000/100000 [12:49<00:00, 129.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posterior mean of beta: [ 6.23374042  0.12301236  0.00664424 -0.01226566 -0.09921363  0.17344778]\n",
      "Posterior mean of eta: 27.235820725709\n",
      "Posterior mean of nu: 22.95943856477037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = site_gamma_df.iloc[:, 0:6].values\n",
    "Y = site_gamma_df.iloc[:, 6].values\n",
    "site_ids = site_gamma_df.iloc[:, 7].values - 2\n",
    "\n",
    "np.random.seed(45)\n",
    "\n",
    "\n",
    "def gibbs_sampling_with_data(\n",
    "    X,\n",
    "    Y,\n",
    "    site_ids,\n",
    "    n_iterations=100000,\n",
    "    beta_prior_mean=None,\n",
    "    beta_prior_cov=None,\n",
    "    eta_prior_shape=2,\n",
    "    eta_prior_scale=1,\n",
    "    nu_prior_shape=2,\n",
    "    nu_prior_scale=1,\n",
    "):\n",
    "    n_data, n_features = X.shape\n",
    "    n_sites = len(np.unique(site_ids))\n",
    "\n",
    "    # Initialize priors\n",
    "    if beta_prior_mean is None:\n",
    "        beta_prior_mean = np.zeros(n_features)\n",
    "    if beta_prior_cov is None:\n",
    "        beta_prior_cov = np.eye(n_features)  # Large prior variance\n",
    "\n",
    "    # Initialize Gibbs sampling arrays\n",
    "    beta_samples = np.zeros((n_iterations, n_features))\n",
    "    eta_samples = np.zeros(n_iterations)\n",
    "    nu_samples = np.zeros(n_iterations)\n",
    "    V_samples = np.zeros((n_iterations, n_sites))\n",
    "\n",
    "    # Initial values based on prior\n",
    "    beta_current = np.random.multivariate_normal(beta_prior_mean, beta_prior_cov)\n",
    "    eta_current = gamma.rvs(a=eta_prior_shape, scale=1 / eta_prior_scale)\n",
    "    nu_current = gamma.rvs(a=nu_prior_shape, scale=1 / nu_prior_scale)\n",
    "    V_current = np.zeros(n_sites) + 0.01\n",
    "\n",
    "    # Gibbs sampler\n",
    "    for i in tqdm(range(n_iterations), desc=\"Gibbs Sampling Progress\"):\n",
    "        # Step i: Draw beta conditioned on V, eta, and data\n",
    "        precision_beta = eta_current * (X.T @ X)\n",
    "        mean_beta = np.linalg.inv(precision_beta) @ (\n",
    "            eta_current * X.T @ (Y - V_current[site_ids])\n",
    "        )\n",
    "        beta_current = np.random.multivariate_normal(\n",
    "            mean_beta, np.linalg.inv(precision_beta)\n",
    "        )\n",
    "        beta_samples[i, :] = beta_current\n",
    "\n",
    "        # Step ii: Draw eta conditioned on V and data\n",
    "        residuals = Y - X @ beta_current - V_current[site_ids]\n",
    "        eta_shape_post = n_data / 2\n",
    "        eta_scale_post = 0.5 * np.sum(residuals**2)\n",
    "        eta_current = gamma.rvs(a=eta_shape_post, scale=1 / eta_scale_post)\n",
    "        eta_samples[i] = eta_current\n",
    "\n",
    "        # Step iii: Draw nu conditioned on V\n",
    "        nu_shape_post = n_sites / 2\n",
    "        nu_scale_post = 0.5 * np.sum(V_current**2)\n",
    "        nu_current = gamma.rvs(a=nu_shape_post, scale=1 / nu_scale_post)\n",
    "        nu_samples[i] = nu_current\n",
    "\n",
    "        # Step iv: Draw V_j conditioned on beta, nu, eta, and data\n",
    "        for j in range(n_sites):\n",
    "            site_mask = site_ids == j\n",
    "            n_j = np.sum(site_mask)\n",
    "            V_bar_j = np.mean(Y[site_mask] - X[site_mask] @ beta_current)\n",
    "            mean_V_j = (n_j * eta_current * V_bar_j) / (n_j * eta_current + nu_current)\n",
    "            precision_V_j = n_j * eta_current + nu_current\n",
    "            V_current[j] = np.random.normal(mean_V_j, np.sqrt(1 / precision_V_j))\n",
    "        V_samples[i, :] = V_current\n",
    "\n",
    "    # Remove burnin samples\n",
    "    beta_samples = beta_samples[30000:, :]\n",
    "    eta_samples = eta_samples[30000:]\n",
    "    nu_samples = nu_samples[30000:]\n",
    "    V_samples = V_samples[30000:, :]\n",
    "\n",
    "    # Compute posterior hyperparams\n",
    "    beta_post_mean = np.mean(beta_samples, axis=0)\n",
    "    beta_post_var = np.cov(beta_samples, rowvar=False, ddof=0)\n",
    "\n",
    "    eta_post_mean = np.mean(eta_samples)\n",
    "    eta_post_var = np.var(eta_samples, ddof=0)\n",
    "\n",
    "    nu_post_mean = np.mean(nu_samples)\n",
    "    nu_post_var = np.var(nu_samples, ddof=0)\n",
    "\n",
    "    V_post_mean = np.mean(V_samples, axis=0)\n",
    "    V_post_var = np.var(V_samples, ddof=0, axis=0)\n",
    "\n",
    "    return (\n",
    "        beta_post_mean,\n",
    "        eta_post_mean,\n",
    "        nu_post_mean,\n",
    "        V_post_mean,\n",
    "        beta_post_var,\n",
    "        eta_post_var,\n",
    "        nu_post_var,\n",
    "        V_post_var,\n",
    "    )\n",
    "\n",
    "\n",
    "(\n",
    "    beta_post_mean,\n",
    "    eta_post_mean,\n",
    "    nu_post_mean,\n",
    "    V_post_mean,\n",
    "    beta_post_var,\n",
    "    eta_post_var,\n",
    "    nu_post_var,\n",
    "    V_post_var,\n",
    ") = gibbs_sampling_with_data(X, Y, site_ids)\n",
    "\n",
    "\n",
    "print(f\"Posterior mean of beta: {beta_post_mean}\")\n",
    "print(f\"Posterior mean of eta: {eta_post_mean}\")\n",
    "print(f\"Posterior mean of nu: {nu_post_mean}\")\n",
    "\n",
    "\n",
    "data = []\n",
    "\n",
    "\n",
    "for i in range(len(beta_post_mean)):\n",
    "    data.append(\n",
    "        {\n",
    "            \"Parameter\": f\"beta_{i+1}\",\n",
    "            \"Posterior Mean\": beta_post_mean[i],\n",
    "            \"Posterior Variance\": None,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "for i in range(len(beta_post_mean)):\n",
    "    for j in range(len(beta_post_mean)):\n",
    "        data.append(\n",
    "            {\n",
    "                \"Parameter\": f\"var_beta_{i+1}_{j+1}\",\n",
    "                \"Posterior Mean\": None,\n",
    "                \"Posterior Variance\": beta_post_var[i, j],\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "data.append(\n",
    "    {\n",
    "        \"Parameter\": \"eta\",\n",
    "        \"Posterior Mean\": eta_post_mean,\n",
    "        \"Posterior Variance\": eta_post_var,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "data.append(\n",
    "    {\n",
    "        \"Parameter\": \"nu\",\n",
    "        \"Posterior Mean\": nu_post_mean,\n",
    "        \"Posterior Variance\": nu_post_var,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "for i in range(len(V_post_mean)):\n",
    "    data.append(\n",
    "        {\n",
    "            \"Parameter\": f\"V_{i+1}\",\n",
    "            \"Posterior Mean\": V_post_mean[i],\n",
    "            \"Posterior Variance\": V_post_var[i],\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.to_csv(data_dir + \"/gamma_posterior_means_and_variances.csv\", index=False)\n",
    "\n",
    "\n",
    "# Fit values\n",
    "\n",
    "beta = np.random.multivariate_normal(beta_post_mean, beta_post_var, 100000)\n",
    "V = np.random.normal(\n",
    "    V_post_mean, np.sqrt(V_post_var), (100000, len(np.unique(site_ids)))\n",
    ")\n",
    "\n",
    "\n",
    "gamma_fit_df_1043 = gpd.read_file(data_dir + \"/gamma_data_site_1043.geojson\")\n",
    "X_fit = gamma_fit_df_1043.iloc[:, 0:6].values\n",
    "id_fit = gamma_fit_df_1043.iloc[:, 7].values - 2\n",
    "gamma_fit_df_1043 = np.mean(np.exp(X_fit @ beta.T + V[:, id_fit ].T), axis=1)\n",
    "gamma_fit_df_1043_df = pd.DataFrame(gamma_fit_df_1043, columns=[\"gamma_fit\"])\n",
    "gamma_fit_df_1043_df.to_csv(data_dir + \"/gamma_fit_1043.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
